URL: https://docs.python.org/3/library/urllib.robotparser.html

Title: 21.10. urllib.robotparser — Parser for robots.txt — Python 3.5.2 documentation

Doc Id: 171

Meta Tags : No meta tag found in document

Date :Jun 27, 2016

Content :Source code: Lib/urllib/robotparser.py
This module provides a single class, RobotFileParser, which answers
questions about whether or not a particular user agent can fetch a URL on the
Web site that published the robots.txt file.  For more details on the
structure of robots.txt files, see http://www.robotstxt.org/orig.html.
This class provides methods to read, parse and answer questions about the
robots.txt file at url.
Sets the URL referring to a robots.txt file.
Reads the robots.txt URL and feeds it to the parser.
Parses the lines argument.
Returns True if the useragent is allowed to fetch the url
according to the rules contained in the parsed robots.txt
file.
Returns the time the robots.txt file was last fetched.  This is
useful for long-running web spiders that need to check for new
robots.txt files periodically.
Sets the time the robots.txt file was last fetched to the current
time.
The following example demonstrates basic use of the RobotFileParser class.
21.9. urllib.error — Exception classes raised by urllib.request
21.11. http — HTTP modules

    Enter search terms or a module, class or function name.
    

